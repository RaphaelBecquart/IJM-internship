{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ToolBox.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "from tools.ToolBox import json_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_classifier(angle, n_class):\n",
    "    \"\"\" Function that transform angle (in degree) into class\n",
    "    Arguments:\n",
    "        -> angle: a angle in degree\n",
    "        -> n_class: the number of expected class\n",
    "    \"\"\"\n",
    "    return int(angle // (360 / n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_aggregator(path = None, problem = None, n_classes = 1):\n",
    "    \"\"\"Arguments\n",
    "        -> path: the path to a folder where .json files are located\n",
    "        -> problem: the nature of the problem. 'classification' or 'regression' regression (None) per default\n",
    "    \"\"\"\n",
    "    \n",
    "    # serach all given .json in a path \n",
    "    if path == None:\n",
    "        file_list = glob('./*.json')\n",
    "    else:\n",
    "        file_list = glob(path + '*.json')\n",
    "\n",
    "    angle_db = []\n",
    "    img_db = []\n",
    "\n",
    "    count = 1\n",
    "    \n",
    "    # for each .json\n",
    "    for a_file in file_list:\n",
    "        data = json_loader(a_file)\n",
    "        \n",
    "        # append each centriole and each corresponding angle to separate list\n",
    "        try:\n",
    "            for a_centriole in data[list(data.keys())[0]]:\n",
    "                if problem == 'classification':\n",
    "                    angle_db.append(angle_classifier(a_centriole['angle'], n_classes ))\n",
    "                else:\n",
    "                    angle_db.append(a_centriole['angle'])\n",
    "\n",
    "                img_db.append(a_centriole['image'])\n",
    "        except:\n",
    "            print('file note treated, data key: {}, file name: {}\\n'.format(data.keys(), file_name))\n",
    "            \n",
    "        print('File treated: {}/{} , Current file: {}\\n'.format(count, len(file_list), list(data.keys())[0]))\n",
    "        count += 1\n",
    "\n",
    "    # transform and reshape the img dataset in array    \n",
    "    img_db = np.array(img_db, dtype = 'double')\n",
    "    img_db = img_db.reshape(img_db.shape[0], 1, img_db.shape[1], img_db.shape[2])  \n",
    "\n",
    "    # return an array containing centrioles images and a list containing angle\n",
    "    return img_db, angle_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class centriole_dataset(Dataset):\n",
    "    def __init__(self, img_db, angle_db, transform = None, root_dir = None, problem = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_db = img_db\n",
    "        self.angle_db = angle_db\n",
    "        self.transform = transform\n",
    "        self.problem = problem\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.angle_db)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.img_db[idx]\n",
    "        angle = self.angle_db[idx]\n",
    "        \n",
    "        if self.problem == 'classification':\n",
    "            angle = np.array(angle, dtype = 'int') # [angle]\n",
    "        else:\n",
    "            angle = np.array(angle, dtype = 'double')\n",
    "        \n",
    "        sample = {'image': img, 'angle': angle}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(path_json = './data_json', batch_size = 700, n_class = 72, save_dataset = False):\n",
    "    \"\"\" a function that create a dataset from appropriate json file\n",
    "    Arguments:\n",
    "        -> path_json  path to the json file\n",
    "        -> batch_size: size of the batch\n",
    "        -> n_class: number of class for classification problem \n",
    "        -> save_dataset: save the created dataset in ./weight/ directory\n",
    "        \n",
    "    The created datasets as property: \n",
    "        test_size = 0.2, \n",
    "        shuffle = True (for train) \n",
    "                  False (for validation)\n",
    "        drop_last = True (for both)\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_class != 1:\n",
    "        problem = 'classification'\n",
    "        \n",
    "    else:\n",
    "        problem  = 'regression'\n",
    "        \n",
    "    img_db, angle_db = dataset_aggregator(path = path_json, problem = problem, n_classes = n_class)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(img_db, angle_db, test_size=0.20)\n",
    "\n",
    "    training = centriole_dataset(img_db = x_train, angle_db = y_train, problem = problem)\n",
    "    testing = centriole_dataset(img_db = x_test, angle_db = y_test, problem = problem)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(training, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(testing, batch_size = batch_size, shuffle = False, drop_last=True)\n",
    "    \n",
    "    if save_dataset == True:\n",
    "        if problem == 'classification':\n",
    "            train_name = './data/train_data_p' + problem + '_n' + str(n_class) + '_b' + str(batch_size) + '_.pth' \n",
    "            val_name = './data/validation_data_p' + problem + '_n' + str(n_class) + '_b' + str(batch_size) + '_.pth' \n",
    "        \n",
    "        else:\n",
    "            train_name = './data/train_data_p' + problem + '_b' + str(batch_size) + '_.pth' \n",
    "            val_name = './data/validation_data_p' + problem + '_b' + str(batch_size) + '_.pth' \n",
    "        \n",
    "        pickle.dump(train_loader, open(train_name, 'wb'), protocol=4)\n",
    "        pickle.dump(validation_loader, open(val_name, 'wb'), protocol=4)\n",
    "            \n",
    "             \n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(path = '../data/', train_set = 'train_loader_dataset_b700_unNormalized.pth', val_set = 'validation_loader_dataset_b700_unNormalized.pth' ):\n",
    "    \"\"\" A function that load a 'torch' dataset.\n",
    "    Arguments:\n",
    "        -> path    : path to the dataset location\n",
    "        -> train_set: name of the train dataset\n",
    "        -> val_set  : name of the validation dataset\n",
    "    \"\"\"\n",
    "    train_loader = pickle.load(open(path + train_set, 'rb'))\n",
    "    validation_loader = pickle.load(open(path + val_set, 'rb'))\n",
    "    \n",
    "    return train_loader, validation_loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
